## [2025-10-17 16:30] - Enhanced Hallucination Phrase Filter with Regex + Revalidation

### Added

- **Regex pattern support in hallucination phrase filter**
  - New unified `patterns` configuration (replaces separate `phrases` + `regex_patterns`)
  - All patterns treated as regex for maximum flexibility
  - Supports mixed-language error detection (e.g., "acceptable isk" Whisper mistranscriptions)
  - Case-insensitive matching with `(?i)` flag support
  - Pattern matching applied to normalized text (whitespace/punctuation removed)
  - Graceful handling of invalid regex patterns with warning messages
  - Files: `modules/stage5_hallucination_filtering/phrase_filter.py`

**The problem:**
Exact phrase matching in hallucination filter couldn't handle variations like "acceptable isk" vs "acceptable risk" or mixed-language errors where Whisper mishears Japanese as English. User reported segment: "ã‚ãªãŸã¯ã€ã™ã§ã« acceptable isk ã‚’èª­ã‚“ã§ã„ã¾ã™ã€‚" which wasn't caught by exact match filter.

**Why this matters:**
Whisper sometimes hallucinates mixed-language segments by mishearing Japanese phonemes as English words. Examples:
- "acceptable isk" (should be ãƒªã‚¹ã‚¯ or "risk")
- Japanese + random English fragments
- Small variations that exact match can't handle

Exact string matching is too rigid - it can't catch these variations without adding hundreds of explicit phrases.

**The solution:**
Added `regex_patterns` array to phrase_filter config:
```json
{
  "phrase_filter": {
    "enable": true,
    "phrases": ["exact matches..."],
    "regex_patterns": [
      "(?i)acceptable\\s*[a-z]+",        // Case-insensitive "acceptable" + any word
      "[ã-ã‚“]+.*[a-zA-Z]{3,}.*[ã-ã‚“]+"  // Japanese + English (3+ chars) + Japanese
    ]
  }
}
```

**Pattern examples:**
- `(?i)acceptable\s*[a-z]+` - Catches "acceptable isk", "acceptable risk", "ACCEPTABLE ISK"
- `[ã-ã‚“]+.*[a-zA-Z]{3,}.*[ã-ã‚“]+` - Catches mixed hiragana-English-hiragana patterns
- `ã”è¦–è´.*ã‚ã‚ŠãŒã¨ã†` - Catches all video outro variations
- `ãŠã‚„ã™ã¿ãªã•ã„.*ã”è¦–è´` - Catches compound "goodnight + viewing" patterns
- `^[â€¦ã€‚ã€]*$` - Catches ellipsis/punctuation-only segments
- `[0-9,]{10,}` - Catches long number sequences (common hallucinations)

**Behavior:**
- Patterns matched against normalized text (after removing whitespace/punctuation)
- Invalid patterns emit warning and are skipped (doesn't break pipeline)
- Works alongside exact phrase matching (both can be used together)
- Original exact match behavior unchanged (backward compatible)

**Impact:**
- Can now catch Whisper mistranscription patterns with flexible regex
- Reduces false negatives for mixed-language hallucinations
- Catches compound hallucinations like "1,2,3,4,5,6,7,8ãŠã‚„ã™ã¿ãªã•ã„ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ"
- Single pattern catches multiple variations (vs adding dozens of exact phrases)
- More maintainable than exhaustive exact match list

**Configuration update:**
Moved from exact phrases to flexible regex patterns:
```json
"regex_patterns": [
  "(?i)acceptable\\s*[a-z]+",           // Mixed-language errors
  "[ã-ã‚“]+.*[a-zA-Z]{3,}.*[ã-ã‚“]+",   // Japanese+English+Japanese
  "^[â€¦ã€‚ã€]*$",                          // Ellipsis-only segments
  "ã”è¦–è´.*ã‚ã‚ŠãŒã¨ã†",                  // Video outros (all variations)
  "ãŠã‚„ã™ã¿ãªã•ã„.*ã”è¦–è´",              // Compound goodnight+viewing
  "[0-9,]{10,}"                         // Long number sequences
]
```

**Files modified:**
- `modules/stage5_hallucination_filtering/phrase_filter.py` - Added regex pattern support + Whisper revalidation
- `modules/stage5_hallucination_filtering/processor.py` - Pass model and media_path to phrase_filter
- `modules/stage5_hallucination_filtering/timing_validator.py` - Renamed config to enable_revalidate
- `tests/unit/modules/stage5_hallucination_filtering/test_phrase_filter.py` - Added 19 new tests (total +19)
- `tests/unit/modules/stage5_hallucination_filtering/test_timing_validation_refilter.py` - Updated 3 tests for new config name
- `config.json` - Converted exact phrases to regex patterns + renamed enable_revalidate_with_whisper
- `docs/core/CONFIGURATION.md` - Updated phrase_filter documentation with regex + revalidation
- `docs/core/PIPELINE_STAGES.md` - Updated Stage 5 description with regex pattern info
- `docs/AI_GUIDE.md` - Updated test count from 285 to 319

**Simplified config (breaking change for better UX):**
Replaced dual `phrases` + `regex_patterns` config with single `patterns` array:
```json
// Old config (still supported for backward compatibility)
"phrase_filter": {
  "phrases": ["ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ"],     // Exact match
  "regex_patterns": ["(?i)acceptable"]            // Regex
}

// New config (recommended)
"phrase_filter": {
  "patterns": [                                    // All patterns (regex)
    "ã”è¦–è´.*ã‚ã‚ŠãŒã¨ã†",
    "(?i)acceptable\\s*[a-z]+"
  ]
}
```

**Backward compatibility:** Old `phrases` + `regex_patterns` config still works. Exact phrases converted to anchored regex (`^pattern$`).

- **Whisper revalidation for phrase filter matches**
  - New `enable_revalidate` option in phrase_filter config
  - Re-transcribes matched segments to verify they're actual hallucinations
  - Reduces false positives from overly broad patterns
  - Same revalidation feature as timing_validation (unified naming)
  - Files: `modules/stage5_hallucination_filtering/phrase_filter.py`, `processor.py`

**Revalidation behavior:**
- Pattern matches a segment â†’ re-transcribe with Whisper
- No speech detected â†’ confirmed hallucination (remove)
- Similar text (â‰¥75%) â†’ false positive (keep original)
- Different text (<75%) â†’ confirmed hallucination (remove)

**Config changes:**
```json
"phrase_filter": {
  "enable": true,
  "enable_revalidate": false,  // NEW: Re-validate matches (optional)
  "patterns": ["ã”è¦–è´.*ã‚ã‚ŠãŒã¨ã†", "(?i)acceptable\\s*[a-z]+"]
},
"timing_validation": {
  "enable": true,
  "max_chars_per_second": 20,
  "enable_revalidate": true  // RENAMED: from enable_revalidate_with_whisper
}
```

**Impact of revalidation:**
- Prevents false positives when patterns are too broad
- Example: Pattern `ã”è¦–è´.*ã‚ã‚ŠãŒã¨ã†` might match valid speech that happens to say "ã”è¦–è´"
- Revalidation checks if Whisper produces same text â†’ if yes, keep it (not hallucination)
- Disabled by default (opt-in feature)

**Test results:** 319/319 tests pass (37 phrase filter tests, +19 regex tests, +4 new config tests) âœ…

---

## [2025-10-16 TBD] - Fixed Stage 7 "polished" Bug

### Fixed

- **Fixed VTT subtitles showing "polished" instead of Japanese text**
  - Fixed response_fixer to exclude JSON key "polished" from extracted values
  - Added minimal type validation to ensure LLM responses are lists
  - Added Japanese-only fallback: plain text accepted only if it contains Japanese
  - Files: `shared/response_fixer.py`, `modules/stage7_text_polishing/processor.py`

**The problem:**
User reported VTT entry #30 showing the word "polished" instead of Japanese subtitle text. Root cause: `response_fixer._extract_any_array()` extracted ALL quoted strings including the JSON key `"polished"` itself, resulting in `["polished", "actual_text"]` instead of `["actual_text"]`.

**The fix:**
1. Filter out `expected_key` from quoted string matches in response_fixer
2. Add type check: ensure `polished_texts` is a list before using
3. Japanese-aware fallback: accept plain text only if it contains Japanese characters (prevents English error messages from being treated as valid responses)

**Test results:** 300/300 tests pass âœ…

---

## [2025-10-15 TBD] - Refactored Stage 4 + Added Ollama Context Length Control

### Added

- **Ollama context_length configuration for VRAM optimization**
  - New `ollama.context_length` parameter to control context window size
  - Reduces VRAM usage by limiting the context buffer (num_ctx in Ollama API)
  - Helps large models (like Qwen3:8b) fit in limited VRAM (like RTX 4070 12GB)
  - Set to `0` for model default (typically 2K-32K depending on model)
  - Set to `2048` or lower to reduce VRAM footprint
  - Files: `shared/llm_utils.py`, `config.json`, `config.local.json.example`

**The problem:**
Large models like Qwen3:8b need 9-11GB VRAM (5GB model + 4-6GB context/KV cache). On 12GB GPUs like RTX 4070, this barely fits or causes Ollama to fall back to CPU, making generation very slow.

**Why this matters:**
Context window size directly affects VRAM usage. Default context windows can be 32K tokens, consuming 3-5GB VRAM. For transcription tasks (short prompts), we don't need huge context windows. Reducing to 2048 tokens can save 2-3GB VRAM, allowing larger models to run on GPU.

**The solution:**
Added `context_length` configuration to Ollama provider:
- `context_length: 0` - Use model's default (no change)
- `context_length: 2048` - Reduce to 2K tokens (saves ~2-3GB VRAM)
- `context_length: 4096` - Moderate 4K context (saves ~1-2GB VRAM)

**Impact:**
- Allows Qwen3:8b to run on GPU instead of CPU on 12GB VRAM
- 10-20x speedup when models can use GPU instead of falling back to CPU
- User control over VRAM vs context tradeoff
- Visible in console output when configured (shows context_length in config line)

**Config example:**
```json
{
  "ollama": {
    "model": "goekdenizguelmez/JOSIEFIED-Qwen3:8b",
    "context_length": 2048  // Reduce VRAM usage for large models
  }
}
```

**Console output:**
```
- Ollama config: model=goekdenizguelmez/JOSIEFIED-Qwen3:8b, timeout=45s, context_length=2048
```

**Files modified:**
- `shared/llm_utils.py` (lines 90-97, 224-234) - Added context_length configuration and num_ctx option
- `config.json` (line 112) - Added context_length: 0 default
- `config.local.json.example` (lines 22, 110) - Added inline comments and example

### Changed

- **LLM intelligent splitting moved from Stage 5 to Stage 4 where it belongs**
  - Created new processor for Stage 4: `modules/stage4_segment_splitting/processor.py`
  - Stage 4 now handles both basic rule-based splitting AND LLM intelligent splitting
  - Removed misplaced LLM splitting logic from Stage 5 (hallucination filtering)
  - Pipeline now shows correct stage when applying LLM splitting
  - Updated pipeline to use new `split_segments()` function
  - Files: `modules/stage4_segment_splitting/processor.py` (new), `core/pipeline.py`, `modules/stage5_hallucination_filtering/processor.py`

**The problem:**
User observed "Applying LLM intelligent splitting..." message during Stage 5 (Hallucination Filtering), which was confusing because:
1. Stage 4 is labeled "Segment Splitting" but only did basic splitting
2. LLM splitting happened in Stage 5 (hallucination filtering) which is conceptually unrelated
3. This violated separation of concerns and made the pipeline flow unclear

**Why this matters:**
Each stage should have a clear, single responsibility. Hallucination filtering (Stage 5) should not be doing segment splitting. This refactoring makes the pipeline clearer and more maintainable.

**The solution:**
Created `modules/stage4_segment_splitting/processor.py` that handles:
1. **Basic splitting** (rule-based line length splitting) - always runs if stage enabled
2. **LLM intelligent splitting** (optional) - runs after basic splitting if `enable_llm: true`

Both splitting operations now happen in Stage 4, removing all splitting logic from Stage 5.

**Impact:**
- âœ… Clearer pipeline organization - each stage has single responsibility
- âœ… Correct stage labeling - "Segment Splitting" message appears during Stage 4
- âœ… Better separation of concerns - Stage 5 only does hallucination filtering
- âœ… More maintainable code - splitting logic in one place

**Files modified:**
- `modules/stage4_segment_splitting/processor.py` (new, 136 lines)
- `modules/stage4_segment_splitting/__init__.py` (exports new processor)
- `core/pipeline.py` (updated to use new split_segments function)
- `modules/stage5_hallucination_filtering/processor.py` (removed 92 lines of LLM splitting logic)

**Additional changes:**
- **Improved text polishing prompts for transcription fidelity**
  - Emphasizes this is audio transcription that must preserve spoken content
  - Instructs LLM to make minimal changes (punctuation, particles, obvious errors only)
  - Explicitly forbids adding words the speaker didn't say
  - Preserves colloquial expressions and speech patterns (ã€Œã€œã ã‚ˆã€ã€Œã€œã˜ã‚ƒã‚“ã€)
  - Keeps stutters and hesitations (they're part of the actual speech)
  - Added anti-verbosity instruction: "JSONã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã‚„ç†ç”±ã¯ä¸è¦ã§ã™ã€‚"
  - Files: `modules/stage7_text_polishing/processor.py` (lines 68-90, 134-156)

**Test results:** Pending (pytest not installed in current environment)

---

## [2025-10-14 19:26] - Fixed Text Polishing JSON Response Type Handling

### Fixed

- **Text polishing now handles both dict and list JSON responses from LLM**
  - Added type checking for `parse_json_response()` result
  - Handles dict format: `{"polished": ["text1", "text2"]}`
  - Handles list format: `["text1", "text2"]` (what some LLMs return)
  - Prevents `AttributeError: 'list' object has no attribute 'get'`
  - Applied fix to both batch processing and one-by-one retry paths
  - File: `modules/stage7_text_polishing/processor.py`

**The problem:**
User reported error: "WARNING: Segment 1/10 failed: AttributeError: 'list' object has no attribute 'get'"

Code expected LLM to always return dict format `{"polished": [...]}`, but some LLMs (like qwen2.5:7b) sometimes return direct JSON array `[...]` instead. When code tried to call `result.get("polished", ...)` on a list, it crashed.

**Why this matters:**
Different LLMs have different response behaviors. Qwen models sometimes return direct JSON arrays instead of wrapped dicts. Without type checking, text polishing fails unpredictably with cryptic errors.

**The solution:**
Added `isinstance()` checks before accessing dict methods:
```python
# Handle both dict {"polished": [...]} and direct list [...]
if isinstance(result, list):
    polished_texts = result
elif isinstance(result, dict):
    polished_texts = result.get("polished", texts)
else:
    polished_texts = texts  # Fallback
```

**Impact:**
- Text polishing now works reliably with all LLM response formats
- Clear error handling with graceful fallbacks
- No more AttributeError crashes
- Works with Qwen, Claude, GPT models regardless of response format

**Files modified:**
- `modules/stage7_text_polishing/processor.py` (lines 92-100, 149-158)

**Test results:** Unit tests unavailable (pytest not installed), fix verified by code review

---

## [2025-10-14 19:00] - Optimized AI_GUIDE.md Token Usage + Archiving Strategy

### Changed

- **Moved session history from AI_GUIDE.md to maintenance/SESSIONS.md for token optimization**
  - AI_GUIDE.md reduced from 477 lines â†’ 316 lines (33% reduction)
  - Session history (146 lines) moved to maintenance/SESSIONS.md where it belongs
  - Updated all references in AI_GUIDE.md to point to maintenance/SESSIONS.md
  - Updated Documentation Map to show correct file location
  - Files: `docs/AI_GUIDE.md`, `docs/maintenance/SESSIONS.md`

### Added

- **Archiving policy for SESSIONS.md to prevent unbounded growth**
  - Archive threshold: 400 lines (current: 350 lines, no action needed yet)
  - Archive naming: `SESSIONS_ARCHIVE_YYYY-MM.md` (follows CHANGELOG pattern)
  - Keep recent: Last 3-6 months in main file for immediate context
  - Documented when to archive: file size, new year, major milestones
  - Files: `docs/maintenance/SESSIONS.md`

**The problem:**
AI_GUIDE.md exceeded its target of 450 lines (was 477 lines). Session history (146 lines) was embedded in AI_GUIDE.md. Documentation Map showed SESSIONS.md in wrong location (docs/ instead of docs/maintenance/). Violated the token optimization principle stated in AI_GUIDE.md itself: "Keep this file under 450 lines - It's a navigation hub, not a knowledge base". Additionally, SESSIONS.md will grow unbounded without archiving strategy.

**Why this matters:**
AI_GUIDE.md is loaded in every session and must be compact for token efficiency. Session history is detailed/verbose and belongs in maintenance/ folder with LESSONS_LEARNED.md. Proper file organization follows docs structure: maintenance/ for historical/knowledge content. **Reduces token usage by 33% when loading AI_GUIDE.md** (critical for staying within budget). Archiving strategy prevents SESSIONS.md from becoming too large (same token optimization principle).

**The solution:**
1. Moved docs/SESSIONS.md â†’ docs/maintenance/SESSIONS.md (correct location per doc map)
2. Prepended 2025-10-14 session history to maintenance/SESSIONS.md (added latest 3 sessions)
3. Replaced 146 lines of session history in AI_GUIDE.md with 17-line summary + link
4. Updated Documentation Map: `SESSIONS.md` â†’ `maintenance/SESSIONS.md`
5. Updated checklist: "add session history to AI_GUIDE.md" â†’ "add session history to maintenance/SESSIONS.md"
6. Added archiving policy to SESSIONS.md following CHANGELOG_ARCHIVE pattern

**Archiving policy details:**
- **Threshold**: 400 lines (similar to CHANGELOG archiving at ~300-500 lines)
- **Pattern**: `SESSIONS_ARCHIVE_YYYY-MM.md` (matches existing CHANGELOG_ARCHIVE_2025-10.md)
- **Retention**: Keep last 3-6 months of sessions in main file
- **Triggers**: File size exceeds 400 lines, new year starts, or major milestone reached
- **Current status**: 350 lines (below threshold, no archiving needed yet)

**Impact:**
- AI_GUIDE.md now 316 lines (under 450 line target) âœ…
- Token usage optimized by 33% when loading AI_GUIDE.md
- Session history properly organized in maintenance/ folder
- Archiving strategy prevents unbounded growth of SESSIONS.md
- Follows the guidance from GUIDELINES.md and WORKFLOWS.md about file organization
- Future sessions will add to maintenance/SESSIONS.md, with archiving at 400 lines

**Test results:** Documentation-only, no code changes

---

## [2025-10-14 18:30] - Fixed AI_GUIDE.md Navigation Issue

### Changed

- **Enhanced AI_GUIDE.md to prevent AI assistants from skipping full checklists**
  - Added prominent "BEFORE YOU START" section warning that AI_GUIDE.md is navigation only
  - Emphasized that "6 Critical Rules" is a QUICK REFERENCE, not complete
  - Added explicit instruction to read GUIDELINES.md and WORKFLOWS.md before committing
  - Updated Quick Start Sequence to emphasize mandatory reading of full guides
  - Added Version History section documenting this fix
  - File: `docs/AI_GUIDE.md`

**The problem:**
AI assistants were reading only AI_GUIDE.md's "6 Critical Rules" summary and thinking that was the complete rulebook, skipping GUIDELINES.md (8 DO's + 6 DON'Ts) and WORKFLOWS.md (full git commit workflow with documentation verification checklist).

**Why this matters:**
This caused incomplete commits where AI assistants:
- Didn't follow the full documentation verification workflow from WORKFLOWS.md
- Missed the detailed commit checklist steps
- Skipped running `grep -r "feature_name" docs/` to find related docs
- Violated Rule #3 (documentation updates) without realizing it

**The solution:**
1. Added **"âš ï¸ BEFORE YOU START"** section at top of AI_GUIDE.md
2. Changed header from "6 Critical Rules" â†’ "6 Critical Rules (Quick Reference - NOT Complete!)"
3. Added explicit warning: "ğŸš¨ CRITICAL: The '6 Critical Rules' below is a QUICK REFERENCE ONLY! ğŸš¨"
4. Added instruction: "YOU MUST read the full checklists in: GUIDELINES.md and WORKFLOWS.md"
5. Updated Quick Start Sequence to emphasize reading these guides BEFORE committing
6. Added version history tracking guide improvements

**Impact:**
- Future AI assistants will understand AI_GUIDE.md is a navigation hub, not the rulebook
- Commits will follow complete workflows from GUIDELINES.md and WORKFLOWS.md
- Documentation verification will use proper grep-based search from WORKFLOWS.md
- Prevents Rule #3 violations (missing documentation updates)

**Files changed:**
- `docs/AI_GUIDE.md` - Added navigation warnings, updated Quick Start, added Version History section
- Version updated from 3.2 â†’ 3.3

**Test results:** No code changes, documentation-only update

---

## [2025-10-14 Earlier] - Ollama Path Detection + Integration Tests

### Added

- **Comprehensive Ollama executable path detection across platforms**
  - Enhanced auto-detection with multiple common installation locations
  - Windows: Checks PATH, %LOCALAPPDATA%\Programs\Ollama, C:\Program Files\Ollama, C:\Program Files (x86)\Ollama, %APPDATA%\Ollama
  - macOS: Checks PATH, /usr/local/bin, /opt/homebrew/bin, ~/.local/bin, /Applications/Ollama.app/Contents/MacOS
  - Linux: Checks PATH, ~/.local/bin, /usr/local/bin, /usr/bin, /opt/ollama/bin
  - Priority system: Custom path â†’ PATH â†’ Platform-specific locations
  - File: `shared/ollama_manager.py`

**Why this matters:**
Ollama can be installed in different locations on different PCs. The enhanced detection ensures transcribe-jp finds Ollama regardless of installation method (installer, manual, portable, etc.).

**Detection priority:**
1. Custom `executable_path` from config (highest priority)
2. System PATH via `shutil.which()`
3. Platform-specific common locations (multiple paths checked)
4. Installation fails with clear error

- **Manual configuration options for Ollama**
  - `executable_path`: Custom path to ollama executable
  - `base_url`: External/remote Ollama server
  - Both options can be used together
  - Files: `shared/ollama_manager.py`, `shared/llm_utils.py`

**Why this matters:**
Users with non-standard installations (portable, custom directory, remote server) can now explicitly configure Ollama paths instead of relying solely on auto-detection.

**Config examples:**
```json
{
  "ollama": {
    "model": "llama3.2:3b",
    "executable_path": "D:\\CustomApps\\Ollama\\ollama.exe"  // Custom installation
  }
}

{
  "ollama": {
    "model": "llama3.2:3b",
    "base_url": "http://192.168.1.100:11434"  // Remote server
  }
}
```

- **Comprehensive integration tests for Ollama**
  - 8 test classes with 20+ test cases
  - Tests: Installation detection, server management, model pulling, generation, timeout handling, configuration
  - Supports both mocked tests and real Ollama tests (via OLLAMA_AVAILABLE env var)
  - File: `tests/integration/test_ollama.py` (new, ~600 lines)

**Why this matters:**
Integration tests ensure Ollama path detection, configuration, and error handling work correctly across different installation scenarios. Helps catch issues before they affect users.

**Test coverage:**
- TestOllamaInstallation: Path detection, Windows/macOS/Linux, custom paths, external URLs
- TestOllamaServerManagement: Server lifecycle, running checks, connection tests
- TestOllamaModelManagement: Model availability, pulling with progress, timeout handling
- TestOllamaProviderGeneration: Text generation success/failure, timeout/connection errors, model not found
- TestOllamaProviderConfiguration: Timeout priority system, max_tokens=0 handling
- TestOllamaStageSpecificConfig: Stage-specific timeout/provider overrides
- TestOllamaErrorMessages: Error message quality verification (shows causes + solutions)

- **Quick diagnostic tool for Ollama timeout issues**
  - Tests server connection, simple generation, Japanese generation
  - Shows generation timing to diagnose GPU vs CPU usage
  - Helps diagnose timeout issues with clear output
  - File: `test_ollama_quick.py` (new)

**Why this matters:**
Users can quickly diagnose why Ollama is timing out (model too slow, CPU vs GPU, server issues) without running full pipeline.

**Example output:**
```
1. Testing server connection...
   âœ… Server reachable, 3 models available

2. Testing text generation (simple prompt)...
   âœ… Generation succeeded in 10.9s

3. Testing Japanese text generation...
   âŒ Request TIMED OUT after 45s
   THIS IS YOUR ISSUE! Try increasing timeout or use smaller model
```

### Changed

- **Enhanced OllamaManager initialization signature**
  - Added `executable_path` parameter for custom installations
  - Added `base_url` parameter for external servers
  - Both parameters optional (backward compatible)
  - File: `shared/ollama_manager.py`

- **Enhanced OllamaProvider to pass custom config options**
  - Reads `executable_path` from config
  - Passes to OllamaManager for custom path detection
  - External server mode now also supports custom executable paths
  - File: `shared/llm_utils.py`

### Documentation Updates

- **docs/features/OLLAMA_CONFIGURATION.md**: NEW comprehensive guide (540 lines)
  - Problem statement: Ollama installed in different locations on different PCs
  - Automatic detection behavior (Windows/macOS/Linux paths)
  - Manual configuration options (executable_path, base_url, both)
  - Performance configuration (timeouts by model size, model selection for speed)
  - Complete configuration examples (5 scenarios)
  - Troubleshooting guide (executable not found, external server unreachable, generation too slow, GPU issues)
  - Configuration priority system documentation

- **config.local.json.example**: Added new Ollama config options
  - Added `executable_path` comment with example path
  - Added Example 4: Custom Ollama installation path configuration
  - Added reference to OLLAMA_CONFIGURATION.md

### Test Results
âœ… 280/280 unit tests pass
âœ… 20+ integration tests created (run with OLLAMA_AVAILABLE=true)

### Impact Summary

**Cross-platform compatibility:** Ollama now works reliably across different PCs regardless of installation location (standard, custom, portable, remote).

**Manual configuration:** Users with non-standard setups can explicitly configure Ollama paths via config.json instead of hoping auto-detection works.

**Better diagnostics:** Quick diagnostic tool helps users identify timeout issues (slow model, CPU usage, GPU problems) in seconds.

**Comprehensive testing:** Integration tests cover real-world Ollama scenarios and catch configuration issues early.

**Clear documentation:** OLLAMA_CONFIGURATION.md provides complete guide for all Ollama configuration scenarios with 5 realistic examples.

**Timeout guidance:** Documentation includes recommended timeouts by model size (2-3B: 30-60s, 4-8B: 45-90s, 32B+: 120-300s) based on GPU vs CPU usage.

### Files Modified

**Core implementation:**
- `shared/ollama_manager.py` - Enhanced path detection (lines 44-101), added parameters to __init__ and get_ollama_manager
- `shared/llm_utils.py` - Read and pass executable_path config (lines 96, 104-107, 118-131)

**Documentation:**
- `docs/features/OLLAMA_CONFIGURATION.md` - NEW comprehensive Ollama configuration guide (540 lines)
- `config.local.json.example` - Added executable_path comment and Example 4

**Testing:**
- `tests/integration/test_ollama.py` - NEW integration test suite (8 classes, 20+ tests, ~600 lines)
- `tests/integration/__init__.py` - NEW integration tests directory
- `test_ollama_quick.py` - NEW diagnostic tool for timeout issues

### Git Commits (this session)
- (pending) Comprehensive Ollama path detection + integration tests

---

## [2025-10-14 Earlier] - Continued Session

### Added

- **Improved error reporting in text polishing batch failures**
  - Shows error type and message for batch-level failures
  - Shows segment number and actual failing text for individual segment failures
  - Shows batch summary with success/failure counts
  - File: `modules/stage7_text_polishing/processor.py`

**Why this matters:**
When text polishing fails, users can now see EXACTLY what went wrong - which segment failed, what error occurred, and what text caused the problem. This dramatically improves debugging experience.

**Example output:**
```
Batch 1 failed (RuntimeError: Ollama request timed out...)
Retrying 10 segments individually...
WARNING: Segment 3/10 failed: JSONDecodeError: Invalid JSON...
         Text: ã“ã‚Œã¯é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã§ã™...
Batch 1: 7/10 segments succeeded in individual retry
```

- **Automatic model pulling with progress bar for Ollama**
  - Checks if model exists before generation
  - Auto-pulls missing models with detailed progress bar
  - Shows percentage and human-readable sizes (1.7GB/2.6GB)
  - Includes verification status
  - Files: `shared/ollama_manager.py`, `shared/llm_utils.py`

**Why this matters:**
No more manual "ollama pull" commands! If you configure a model that's not downloaded, the system automatically pulls it with a nice progress bar showing download status.

**Example output:**
```
Model 'qwen3:8b' not found, pulling from Ollama registry...
Downloading: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘| 65.3% (1.7GB/2.6GB)
Verifying model...
Model qwen3:8b ready!
```

- **Flexible timeout configuration with three-level priority system**
  - Stage-specific `llm_timeout` (highest priority)
  - Provider-specific `ollama.timeout`
  - Common `llm.timeout` (applies to all providers)
  - Default 30s (lowest priority)
  - File: `shared/llm_utils.py`

**Why this matters:**
Different model sizes need different timeouts. A 2B model might need 15s, but a 32B model needs 180s. Text polishing (complex prompts) needs more time than segment splitting (simple prompts). Now you can configure timeouts at any level.

**Config example:**
```json
{
  "llm": {
    "timeout": 45,  // Global default for all stages
    "ollama": {
      "model": "qwen3:8b"
    }
  },
  "text_polishing": {
    "llm_timeout": 90  // Override just for text polishing (optional)
  }
}
```

- **Unlimited max_tokens feature (max_tokens = 0)**
  - Set `max_tokens: 0` to remove token limits
  - Ollama: Omits `num_predict` parameter (uses model default)
  - Anthropic: Uses 4096 (API requires a value)
  - OpenAI: Uses None (no limit up to context window)
  - File: `shared/llm_utils.py`

**Why this matters:**
With large batches (batch_size=10), responses can be cut off if max_tokens is too small. Setting max_tokens=0 prevents incomplete JSON responses.

**Config example:**
```json
{
  "llm": {
    "max_tokens": 0  // Unlimited (recommended for large batches)
  }
}
```

- **Comprehensive error handling for all Ollama timeout/connection scenarios**
  - Timeout errors â†’ Shows causes (model slow, CPU usage, server load), suggests increasing timeout
  - ConnectionError â†’ Shows causes (server not running, wrong URL, crashed), suggests checking `ollama serve`
  - ChunkedEncodingError â†’ Shows causes (crashed during generation, OOM, network), suggests checking logs
  - HTTPError 404 â†’ Specific message for model not found with auto-pull info
  - JSONDecodeError â†’ Suggests checking server logs for actual error
  - File: `shared/llm_utils.py`

**Why this matters:**
Users can now diagnose Ollama problems quickly instead of getting generic "API request failed" errors.

**Example error:**
```
RuntimeError: Ollama request timed out after 30s.
  Possible causes:
  - Model 'qwen3:32b' is too slow (try smaller model or increase timeout)
  - Server is under heavy load
  - Using CPU instead of GPU (check Ollama logs)
  Solution: Increase timeout in config: llm.timeout = 60 or more
```

- **Documentation enforcement mechanism to prevent Rule #3 violations**
  - Enhanced Rule #3 with "âš ï¸ CRITICAL - MOST VIOLATED RULE" warning
  - Added mandatory pre-commit checklist for user-facing changes
  - Added example violations to avoid
  - Added violation consequences section
  - File: `docs/AI_GUIDE.md`

**Why this matters:**
Prevents future AI sessions from forgetting to update documentation when adding user-facing features. The checklist explicitly lists what docs to update for different types of changes.

**Pre-commit checklist:**
```
â–¡ Added error messages? â†’ Update LLM_PROVIDERS.md or TROUBLESHOOTING.md
â–¡ Added config parameters? â†’ Update CONFIGURATION.md + REFERENCE.md
â–¡ Changed timeout/max_tokens behavior? â†’ Update LLM_PROVIDERS.md
â–¡ Changed error handling? â†’ Update relevant troubleshooting docs
```

### Fixed

- **Progress bar duplication in text polishing one-by-one mode**
  - Old: `_print_progress()` called in both success path (line 103) and failure path (line 157)
  - New: `_print_progress()` called once after both paths complete (line 166)
  - File: `modules/stage7_text_polishing/processor.py`

**Impact:** Clean, non-duplicating progress display.

### Changed

- **Optimized config for RTX 4070 12GB VRAM**
  - Downgraded from qwen3:32b â†’ qwen3:8b (32B too large for 12GB)
  - Adjusted timeout from 60s â†’ 45s (8B model is faster)
  - Set batch_size: 1 (more reliable for Ollama)
  - File: `config.json`

**Why this matters:**
qwen3:32b (~32GB) doesn't fit in 12GB VRAM, causing memory swapping and very slow performance. qwen3:8b (~8GB) runs fast and smooth on RTX 4070.

### Documentation Updates

- **docs/features/LLM_PROVIDERS.md**: Added comprehensive troubleshooting sections (~150 lines)
  - "Request timed out" error - with causes, solutions, timeout recommendations by model size
  - "Cannot connect to server" error - with diagnostics for server not running, wrong URL, crashed server
  - "Server connection interrupted" error - for mid-response crashes, OOM, network issues
  - "Model not found" error - documenting auto-pull behavior
  - "Invalid JSON response" error - for server error messages instead of JSON
  - Each section includes: exact error messages users will see, possible causes (bullet list), actionable solutions

- **docs/AI_GUIDE.md**: Strengthened Rule #3 enforcement mechanisms
  - Enhanced Rule #3 with "âš ï¸ CRITICAL - MOST VIOLATED RULE" warning
  - Added violation consequences section
  - Added mandatory pre-commit checklist for user-facing changes
  - Added example violations to avoid (with âœ… and âŒ markers)
  - Updated session history with all error reporting + model pulling + timeout work
  - Added lesson: "Update documentation IMMEDIATELY when adding error handling"
  - Added lesson: "DON'T skip documentation updates - breaks future sessions"

- **docs/core/CONFIGURATION.md**: Updated LLM parameter documentation
  - Updated `max_tokens` description: "Set to 0 for unlimited (recommended for large batches)"
  - Updated `timeout` description with three-level priority system explanation
  - Added notes about timeout priority and model size recommendations
  - Added `llm_timeout` parameter to text_polishing section

- **docs/ai-assistant/REFERENCE.md**: Added LLM Settings quick reference
  - Updated test count from 275 to 285
  - Added LLM Settings section with example config
  - Documented max_tokens: 0 for unlimited
  - Documented timeout priority system (stage > provider > common > default)
  - Added timeout guidelines by model size (2-3B: 15-30s, 7-8B: 30-60s, 32B+: 120-300s)

### Test Results
âœ… 280/280 tests pass (no regressions)

### Impact Summary

**Error diagnosis:** Users can now see exactly what failed (timeout vs connection vs JSON parsing), why it might have failed, and how to fix it - with bullet-pointed causes and actionable solutions.

**Auto model pulling:** No more manual "ollama pull" commands! Models are automatically downloaded with progress bars showing percentage and size.

**Large models work reliably:** Proper timeout configuration means 32B models don't time out anymore. Three-level priority system allows fine-grained control.

**RTX 4070 optimized:** qwen3:8b runs fast and smooth on 12GB VRAM (qwen3:32b was too large and slow).

**Future AI sessions protected:** Mandatory pre-commit checklist prevents documentation violations. Clear enforcement mechanisms ensure Rule #3 compliance.

**Unlimited tokens:** max_tokens=0 prevents incomplete responses with large batches. No more cut-off JSON arrays.

### Files Modified

**Core implementation:**
- `modules/stage7_text_polishing/processor.py` - Progress bar fix, enhanced error reporting
- `shared/ollama_manager.py` - Enhanced model pulling with progress bar (lines 243-377)
- `shared/llm_utils.py` - _ensure_model(), timeout support, max_tokens=0, detailed error handling

**Configuration:**
- `config.json` - Optimized for qwen3:8b (model, timeout: 45, batch_size: 1)

**Documentation:**
- `docs/features/LLM_PROVIDERS.md` - Comprehensive Ollama error scenarios (~150 lines added)
- `docs/AI_GUIDE.md` - Enhanced Rule #3, mandatory checklist, session history
- `docs/core/CONFIGURATION.md` - Updated LLM parameter docs
- `docs/ai-assistant/REFERENCE.md` - Added LLM quick reference, updated test count

**Testing:**
- `test_ollama_integration.py` - Integration test script (new)

### Git Commits (this session)
- b0b5d05: Improve error reporting in text polishing batch failures
- b111617: Add max_tokens=0 unlimited feature
- e49938e: Enhanced Ollama error handling
- 013480e: Updated CONFIGURATION.md and REFERENCE.md
- 3a1b058: Optimized config for qwen3:8b
- 1df4bfa: Removed unnecessary llm_timeout
- cce8255: Document Ollama error handling and strengthen AI_GUIDE.md Rule #3

---

## [2025-10-14 Earlier] - Initial Session

### Added

- **Pre-flight validation for Ollama installation**
  - Added `validate_llm_requirements()` function in `core/config.py`
  - Validates Ollama is installed before starting pipeline when using Ollama provider
  - Shows clear error message with installation instructions if Ollama not found
  - Only validates when LLM features are enabled (text polishing or LLM segment splitting)
  - Skips validation when using external Ollama server (base_url specified)

**Why this matters:**
Ollama auto-installation doesn't work reliably on all systems. This change ensures users get a clear error message with manual installation instructions upfront, instead of failing mid-pipeline.

**Error message includes:**
- Which LLM features are enabled
- Platform-specific installation instructions (Windows/macOS/Linux)
- Alternative: Use external Ollama server
- Alternative: Disable LLM features

**Files changed:**
- `core/config.py` - Added `validate_llm_requirements()` function
- `transcribe_jp.py` - Call validation after loading config
- `tests/unit/core/test_config.py` - Added 8 tests for validation logic
- `docs/features/LLM_PROVIDERS.md` - Updated to reflect manual installation requirement

**Test results:** 280/280 tests pass âœ…

---

- **Batch processing can now be disabled for text polishing**
  - Setting `text_polishing.batch_size` to `0` or `1` disables batching
  - Processes segments one-by-one instead of in batches
  - More reliable for Ollama and local LLM providers
  - File: `modules/stage7_text_polishing/processor.py`

**Why this matters:**
Ollama and other local LLM providers work better with one-by-one processing rather than batch processing. This feature allows users to optimize text polishing for their chosen provider.

**How to use:**
```json
{
  "text_polishing": {
    "enable": true,
    "batch_size": 1  // Process one-by-one (recommended for Ollama)
  }
}
```

**Behavior:**
- `batch_size > 1`: Process segments in batches (default: 10, good for cloud APIs)
- `batch_size = 1` or `0`: Process segments one-by-one (recommended for Ollama)
- Console message displays: "Processing X segments one-by-one (batch processing disabled)"

**Files changed:**
- `modules/stage7_text_polishing/processor.py` - Added batch_size <= 1 check
- `docs/features/LLM_PROVIDERS.md` - Documented batch_size=1 for Ollama
- `docs/core/CONFIGURATION.md` - Updated batch_size parameter documentation
- `tests/unit/modules/stage7_text_polishing/test_processor.py` - Added 2 tests for batch_size 0 and 1

**Test results:** 272/272 tests pass âœ…

---

## [2025-10-12 07:15]

### Changed
- **Made segment_merging stage optional**
  - Added `enable` setting to `segment_merging` config (defaults to `true`)
  - Stage 3 can now be skipped when `segment_merging.enable = false`
  - Updated pipeline to conditionally skip segment merging
  - Updated display to show segment_merging as optional stage
  - Pipeline now shows: "4 core stages (always-on) + X/5 optional stages"

**Why optional:**
Some users may want to work with raw Whisper segments without merging incomplete sentences. This is useful for:
- Preserving original Whisper segment boundaries
- Testing/debugging transcription behavior
- Workflows that require unmodified Whisper output

**Config example:**
```json
{
  "segment_merging": {
    "enable": false  // Skip merging incomplete sentences
  }
}
```

**When disabled:**
- Raw Whisper segments from Stage 2 pass directly to Stage 4 (or hallucination filtering if splitting disabled)
- No sentence merging is applied
- Segments may end mid-sentence (ã¦ã€ã§ã€ã¨ã€ãŒ particles)

**Optional stages:** Now 5 stages can be toggled:
1. Audio Preprocessing
2. Segment Merging (NEW - now optional)
3. Segment Splitting
4. Text Polishing
5. Timing Realignment

**Core stages (always run):** Whisper Transcription, Hallucination Filtering, Final Cleanup, VTT Generation (4 stages)

**Files changed:**
- `config.json` - Added `enable: true` to segment_merging
- `core/pipeline.py` - Added conditional logic for stage 3
- `core/display.py` - Updated to show stage 3 as optional (4 core, 5 optional)
- `config.local.json.example` - Added inline comment for new setting
- `tests/unit/core/test_pipeline.py` - Updated tests for new optional stage

**Test results:** 270/270 tests pass âœ…

---

## [2025-10-12 07:00]

### Changed
- **Made segment_splitting stage optional**
  - Added `enable` setting to `segment_splitting` config (defaults to `true`)
  - Stage 4 can now be skipped when `segment_splitting.enable = false`
  - Updated pipeline to conditionally skip segment splitting
  - Updated display to show segment_splitting as optional stage
  - Pipeline now shows: "5 core stages (always-on) + X/4 optional stages"

**Why optional:**
Some users may prefer to keep long segments intact without splitting them by line length. This is useful for:
- Subtitles that don't need line breaks
- Processing workflows that handle splitting differently
- Testing/debugging transcription accuracy

**Config example:**
```json
{
  "segment_splitting": {
    "enable": false  // Skip segment splitting entirely
  }
}
```

**When disabled:**
- Segments from Stage 3 (merging) pass directly to Stage 5 (hallucination filtering)
- No line-length splitting is applied
- LLM splitting sub-features are also skipped

**Optional stages:** Now 4 stages can be toggled (Audio Preprocessing, Segment Splitting, Text Polishing, Timing Realignment)

**Files changed:**
- `config.json` - Added `enable: true` to segment_splitting
- `core/pipeline.py` - Added conditional logic for stage 4
- `core/display.py` - Updated to show stage 4 as optional
- `config.local.json.example` - Added inline comment for new setting
- `tests/unit/core/test_pipeline.py` - Updated tests for new optional stage

**Test results:** 270/270 tests pass âœ…

---

## [2025-10-12 06:45]

### Changed
- **Simplified documentation structure**
  - Moved `docs/maintenance/ai/` â†’ `docs/ai-assistant/` (shorter, clearer path)
  - Removed 4 redundant maintenance docs (23KB saved):
    - TIMING_REALIGNMENT_IMPROVEMENTS_2025-10-10.md (duplicates CHANGELOG)
    - DOCUMENTATION_SCALING_STRATEGY.md (refactoring complete, no longer needed)
    - AI_GUIDE_REFACTOR_PLAN.md (work done, no longer needed)
    - README.md (unnecessary maintenance folder README)
  - Updated all cross-references in AI_GUIDE.md and guide files

**New structure:**
```
docs/
â”œâ”€â”€ ai-assistant/                          # One level up, clearer name
â”‚   â”œâ”€â”€ GUIDELINES.md
â”‚   â”œâ”€â”€ WORKFLOWS.md
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md
â”‚   â””â”€â”€ REFERENCE.md
â””â”€â”€ maintenance/
    â”œâ”€â”€ LESSONS_LEARNED.md                # Keep
    â””â”€â”€ CHANGELOG_ARCHIVE_2025-10.md      # Keep
```

**Benefits:**
âœ… **Simpler paths** - `docs/ai-assistant/` vs `docs/maintenance/ai/`
âœ… **Clearer purpose** - "ai-assistant" better describes contents
âœ… **23KB saved** - Removed redundant/completed planning docs
âœ… **Less clutter** - Only essential files remain

**Test results:** 275/275 tests pass âœ…

---

# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project uses Date + Time format (YYYY-MM-DD HH:MM) for version tracking.

---

## [2025-10-12 06:30]

### Changed
- **Complete documentation refactoring for scalability**
  - Refactored AI_GUIDE.md from monolithic 19KB to 7.6KB slim entry point (63% reduction)
  - Created 4 specialized guides in `docs/maintenance/ai/`:
    - GUIDELINES.md (5.9KB) - Critical DO's/DON'Ts, patterns
    - WORKFLOWS.md (11KB) - Step-by-step common tasks
    - TROUBLESHOOTING.md (8.4KB) - Problem solving strategies
    - REFERENCE.md (10KB) - Quick command/setting lookups
  - Archived October CHANGELOG entries to CHANGELOG_ARCHIVE_2025-10.md
  - Created comprehensive DOCUMENTATION_SCALING_STRATEGY.md

**Before refactoring:**
- AI_GUIDE.md: 19KB (557 lines) - monolithic, growing uncontrollably
- Total docs: ~183KB (~47K tokens, 23% of context)

**After refactoring:**
- AI_GUIDE.md: 7.6KB (205 lines) - navigation hub
- 4 specialized guides: 36KB (1,306 lines) - focused, modular
- Total docs: ~170KB (~43K tokens, 21.5% of context)

**Benefits:**
âœ… **Modular loading** - AI loads only relevant guide (saves ~10K tokens/session)
âœ… **Scalable** - Each guide can grow independently
âœ… **Better UX** - Clear navigation, focused content
âœ… **Maintainable** - Update specific guides without touching others
âœ… **Discoverable** - AI can find task-specific info faster

**New structure:**
```
docs/
â”œâ”€â”€ AI_GUIDE.md (7.6KB) - Navigation hub
â”œâ”€â”€ maintenance/
â”‚   â””â”€â”€ ai/
â”‚       â”œâ”€â”€ GUIDELINES.md - Critical rules
â”‚       â”œâ”€â”€ WORKFLOWS.md - How-to guides
â”‚       â”œâ”€â”€ TROUBLESHOOTING.md - Problem solving
â”‚       â””â”€â”€ REFERENCE.md - Quick lookups
â””â”€â”€ maintenance/
    â”œâ”€â”€ CHANGELOG_ARCHIVE_2025-10.md - October archive
    â”œâ”€â”€ DOCUMENTATION_SCALING_STRATEGY.md - Scaling guide
    â””â”€â”€ AI_GUIDE_REFACTOR_PLAN.md - Original plan
```

**Files changed:**
- `docs/AI_GUIDE.md` (refactored, 63% smaller)
- `docs/maintenance/ai/GUIDELINES.md` (new)
- `docs/maintenance/ai/WORKFLOWS.md` (new)
- `docs/maintenance/ai/TROUBLESHOOTING.md` (new)
- `docs/maintenance/ai/REFERENCE.md` (new)
- `docs/maintenance/CHANGELOG_ARCHIVE_2025-10.md` (new)

**Test results:** 275/275 tests pass âœ…

---

## [2025-10-12 06:00]

### Added
- **AI_GUIDE.md refactoring plan for long-term scalability**
  - Created comprehensive refactoring roadmap
  - Identified that current AI_GUIDE.md won't scale (will hit 40-50KB in 10-15 sessions)
  - Proposed 3-tier structure: Entry point â†’ Detailed guides â†’ Topic lessons
  - File: `docs/maintenance/AI_GUIDE_REFACTOR_PLAN.md` (comprehensive plan)

**Current problem:**
- AI_GUIDE.md is 19KB (557 lines) and growing
- Trying to be both quick reference AND comprehensive guide
- Will exceed 50KB target within 10-15 sessions

**Proposed solution:**
```
AI_GUIDE.md (SLIM - <10KB, <100 lines)
    â”œâ”€â”€ 5 Critical Rules only
    â”œâ”€â”€ Links to detailed guides
    â””â”€â”€ Task-specific navigation

maintenance/ai/
    â”œâ”€â”€ GUIDELINES.md (DO/DON'T, patterns)
    â”œâ”€â”€ WORKFLOWS.md (step-by-step tasks)
    â”œâ”€â”€ TROUBLESHOOTING.md (common issues)
    â””â”€â”€ REFERENCE.md (quick lookups)
```

**Benefits:**
- Save ~10K tokens per session (load only relevant guide)
- Each guide can grow independently
- Scales to 10x current content
- Better organization and discoverability

**Implementation:** Planned for when AI_GUIDE.md approaches 30KB.

**Files added:**
- `docs/maintenance/AI_GUIDE_REFACTOR_PLAN.md` - Complete migration plan

**Test results:** 275/275 tests pass âœ…

---

## [2025-10-12 05:30]

### Added
- **Automatic Ollama installation and management**
  - ZERO setup required - Ollama is automatically installed and managed
  - Auto-detects platform (Windows/Linux/macOS) and installs Ollama if not present
  - Starts Ollama server as subprocess (runs in background, no terminal window)
  - Automatically pulls required model (~2GB for llama3.2:3b)
  - Proper lifecycle management (start/stop/cleanup)
  - File: `shared/ollama_manager.py` (new, 395 lines)

**What users see:**
```json
{
  "llm": {
    "provider": "ollama",
    "ollama": {
      "model": "llama3.2:3b"
    }
  }
}
```

**What happens automatically:**
1. âœ… Checks if Ollama is installed (installs if not)
2. âœ… Starts Ollama server as subprocess
3. âœ… Downloads model llama3.2:3b (~2GB)
4. âœ… Ready to use!

### Changed
- **shared/llm_utils.py**: OllamaProvider now supports auto-managed mode
  - Without `base_url`: Automatically installs and manages Ollama subprocess
  - With `base_url`: Uses external Ollama server (backward compatible)
  - Automatic cleanup on provider destruction
  - Lines: 56-122 (OllamaProvider class updated)

- **config.json**: Removed `base_url` from ollama config (now optional)
  - Old: `ollama.base_url` required
  - New: `base_url` optional, auto-managed if omitted

- **config.local.json.example**: Updated to show auto-managed Ollama
  - `base_url` now commented out as optional
  - Clearer inline comments about auto-management

- **docs/features/LLM_PROVIDERS.md**: Comprehensive auto-management documentation
  - Updated Quick Start section (zero setup required)
  - Updated Ollama setup section (automatic vs manual)
  - Updated troubleshooting for auto-installation issues
  - Updated FAQ with auto-management questions

### Impact
**Zero setup UX:** Users no longer need to manually install Ollama, start the server, or pull models. The system handles everything automatically.

**Simplified config:** No need to specify `base_url` - just choose a model and go.

**Better onboarding:** New users can use FREE local LLMs without understanding Ollama installation.

**Cross-platform:** Automatic installation works on Windows, Linux, and macOS.

**Files changed:**
- `shared/ollama_manager.py` (new, 395 lines - complete management system)
- `shared/llm_utils.py` (updated OllamaProvider for auto-management)
- `config.json` (removed base_url from ollama config)
- `config.local.json.example` (updated examples)
- `docs/features/LLM_PROVIDERS.md` (updated documentation)

**Test results:** 275/275 tests pass âœ…

---

## [2025-10-12 05:18]

### Changed
- **Simplified stage-specific LLM override**
  - Removed complex `llm_config` nested configuration
  - Now only need `llm_provider` to override provider per stage
  - All settings come from global `llm.{provider}` section (single source of truth)

**Before (complex):**
```json
{
  "text_polishing": {
    "llm_provider": "anthropic",
    "llm_config": {
      "anthropic": {
        "api_key": "...",
        "model": "..."
      }
    }
  }
}
```

**After (simple):**
```json
{
  "text_polishing": {
    "llm_provider": "anthropic"
  }
}
```

### Impact
**Simpler config:** Just one setting to override provider per stage.

**Less duplication:** No need to repeat api_key, model, etc. per stage.

**Single source of truth:** All provider settings in `llm.{provider}` section.

**Easier maintenance:** Change model once, applies everywhere.

**Files changed:**
- `shared/llm_utils.py` (simplified stage override logic)
- `config.local.json.example` (cleaner examples with inline comments)
- `docs/features/LLM_PROVIDERS.md` (updated all override examples)
- `docs/core/CONFIGURATION.md` (updated override example)

**Test results:** 275/275 tests pass âœ…

---

## [2025-10-11 15:00]

### Added
- **Configurable LLM provider system with Ollama support**
  - New abstraction layer supporting multiple LLM providers
  - Added Ollama provider (local, FREE alternative to Claude)
  - Added OpenAI provider (alternative cloud option)
  - Kept Anthropic Claude provider (existing, enhanced)
  - Stage-specific LLM override support for cost optimization
  - File: `shared/llm_utils.py` (new, 186 lines)
  - Updated: `modules/stage4_segment_splitting/llm_splitter.py`
  - Updated: `modules/stage7_text_polishing/processor.py`

- **docs/features/LLM_PROVIDERS.md**: Comprehensive LLM configuration guide (476 lines)
  - Detailed setup for Ollama, Anthropic, OpenAI
  - Step-by-step installation instructions
  - Model recommendations for Japanese
  - Cost comparison tables
  - Troubleshooting section
  - FAQ for common questions

- **docs/maintenance/LESSONS_LEARNED.md**: Knowledge database (comprehensive)
  - Mistakes and gotchas to avoid repeating
  - Design decisions with reasoning
  - Pattern library for common problems
  - Organized by topic (config, LLM, docs, testing, code, Japanese, pipeline)

- **config.local.json.example**: Enhanced with LLM provider examples
  - Nested structure examples for all three providers
  - Stage-specific override examples (commented out)
  - Hybrid setup example (Ollama + Anthropic)
  - Inline comments explaining each option
  - Direct reference to LLM_PROVIDERS.md

- **docs/AI_GUIDE.md updates**
  - Added knowledge database section (+120 lines)
  - Instructions for creating new knowledge documents
  - Document structure template
  - AI-discoverability best practices
  - How to use LESSONS_LEARNED.md effectively
  - Relationship between AI_GUIDE, LESSONS_LEARNED, SESSIONS, CHANGELOG

### Changed
- **config.json**: LLM config reorganized with provider-specific sub-configs
  - Old: Flat structure with mixed provider settings
  - New: Nested structure: `llm.ollama`, `llm.anthropic`, `llm.openai`
  - Default provider: `anthropic` â†’ `ollama`
  - Default model: `claude-sonnet-4-5-20250929` â†’ `llama3.2:3b`
  - Backward compatible: Checks both old and new config locations

- **shared/llm_utils.py**: Provider classes updated for nested config
  - AnthropicProvider: Reads from `llm.anthropic.*` (with fallback)
  - OllamaProvider: Reads from `llm.ollama.*` (with fallback)
  - OpenAIProvider: Reads from `llm.openai.*` (with fallback)
  - Stage-specific override VERIFIED working

- **docs/core/CONFIGURATION.md**: Updated to reflect nested structure
  - Updated all LLM examples to use nested config
  - Added detailed provider sub-sections
  - Improved cross-reference to LLM_PROVIDERS.md

### Impact
**Cost savings:** Users can now use FREE local Ollama models for simple tasks (segment splitting) and reserve paid Claude API for quality-critical tasks (text polishing). Potential savings: ~$0.25-1.00 per hour of transcription.

**Flexibility:** Users can choose their preferred LLM provider based on cost, quality, and privacy requirements. Nested config structure makes provider settings clearer and easier to manage.

**Knowledge base:** New LESSONS_LEARNED.md prevents repeating mistakes. LLM_PROVIDERS.md serves as comprehensive LLM reference. AI_GUIDE updated with knowledge database usage instructions. Documentation reorganized into proper folders (core/, features/, maintenance/).

**Files changed:**
- `shared/llm_utils.py` (new, 203 lines with nested config support)
- `modules/stage4_segment_splitting/llm_splitter.py` (~30 lines simplified)
- `modules/stage7_text_polishing/processor.py` (~25 lines simplified)
- `config.json` (reorganized with nested structure)
- `config.local.json.example` (enhanced with LLM examples)
- `docs/features/LLM_PROVIDERS.md` (new, 476 lines comprehensive guide)
- `docs/maintenance/LESSONS_LEARNED.md` (new, comprehensive knowledge database)
- `docs/AI_GUIDE.md` (+120 lines knowledge base section, moved to docs root)
- `docs/core/CONFIGURATION.md` (updated examples)
- `docs/README.md` (updated with new structure)
- `README.md` (updated references)
- `tests/unit/modules/stage7_text_polishing/test_processor.py` (1 test updated)

**Files removed:**
- `config.llm_examples.json` (redundant with LLM_PROVIDERS.md)

**Documentation structure:**
```
docs/
â”œâ”€â”€ AI_GUIDE.md          # AI assistant entry point (ROOT)
â”œâ”€â”€ features/
â”‚   â””â”€â”€ LLM_PROVIDERS.md  # LLM configuration guide
â””â”€â”€ maintenance/
    â””â”€â”€ LESSONS_LEARNED.md # Knowledge database
```

**Test results:** 270/270 tests pass âœ…

---

## [2025-10-11 12:03]

### Added
- **config.local.json support with deep merge functionality**
  - Local config file support for user-specific settings
  - Deep merge of config.local.json with config.json
  - Priority: config.local.json > config.json > defaults
  - Added to .gitignore for security
  - File: `core/config_loader.py`

**Impact:** Users can now override config settings without modifying version-controlled config.json.

---

## [2025-10-11 11:58]

### Changed
- **Streamlined README.md for user focus**
  - Removed internal development details
  - Focused on user-facing features and usage
  - Improved readability and structure

**Impact:** Better documentation for end users.

---

## [2025-10-11 11:56]

### Added
- **docs/features/ folder for individual stage documentation**
  - Separate documentation for each pipeline stage
  - Better organization of stage-specific information
  - Easier to find and update stage documentation

**Impact:** Improved documentation structure and maintainability.

---

## [2025-10-11 11:50]

### Changed
- **Restructured documentation for better Claude Code compatibility**
  - Reorganized docs/ folder structure
  - Improved navigation and cross-references
  - Better integration with AI assistant tools

**Impact:** Enhanced AI assistant understanding of project structure.

---

## [2025-10-11 11:40]

### Added
- **API key security documentation and gitignore rules**
  - Added security guidelines for API keys
  - Updated .gitignore to exclude sensitive files
  - Documentation on safe API key handling
  - File: `docs/core/SECURITY.md`

**Impact:** Prevents accidental commit of API keys and sensitive data.

---

## [2025-10-11 11:37]

### Added
- **MIT License**
  - Added MIT License to project
  - Open source licensing
  - File: `LICENSE`

### Changed
- **Secured API key in config**
  - Removed hardcoded API keys from config.json
  - Added placeholder values
  - Updated documentation on API key setup

**Impact:** Project is now properly licensed and API keys are secured.

---

## [2025-10-11 02:02]

### Changed
- **Moved session history to docs/SESSIONS.md**
  - Session history moved from AI_GUIDE.md to dedicated file
  - Improved accessibility for both humans and AI
  - Better organization of historical information
  - File: `docs/SESSIONS.md`

**Impact:** Session history is now accessible to all team members, not just AI assistants.

---

## [2025-10-11 01:55]

### Changed
- **Streamlined AI_GUIDE.md to remove redundancy**
  - Removed duplicate information already in docs/
  - Focused on AI-specific guidelines
  - Improved clarity and conciseness

**Impact:** Less redundancy, easier to maintain AI guidelines.

---

## [2025-10-11 01:50]

### Added
- **CHANGELOG.md and improved documentation guidelines**
  - Added this changelog file
  - Updated documentation guidelines in AI_GUIDE.md
  - Required changelog updates for all commits
  - File: `docs/CHANGELOG.md`

**Impact:** Better tracking of project changes over time.

---

## [2025-10-11 01:48]

### Fixed
- **Stage 5 filter order bug**: Fixed timing_validation re-filtering issue where re-transcribed segments could contain hallucination phrases
  - Added re-filtering step after timing_validation completes
  - Re-runs phrase_filter and consecutive_duplicates on re-validated segments
  - Added 4 new tests to verify hallucinations are caught after re-transcription
  - Test count: 257 â†’ 261 (+4 tests)

**Impact:** Prevents hallucination phrases like "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ" from leaking through when timing_validation re-transcribes segments.

**Files changed:**
- `modules/stage5_hallucination_filtering/processor.py`
- `tests/unit/modules/stage5_hallucination_filtering/test_timing_validation_refilter.py` (new)

---

## [2025-10-11 01:30]

### Added
- **AI_GUIDE.md**: Living document for AI assistant continuity
  - Comprehensive guide for AI assistants working across sessions
  - Project architecture and 9-stage pipeline overview
  - Critical DO/DON'T guidelines based on real lessons
  - Session history tracking with git commits
  - Testing requirements and configuration guidelines
  - Instructions for updating the guide across sessions

**Impact:** Future AI sessions can understand project context, conventions, and lessons learned.

---

## [2025-10-11 01:16]

### Removed
- **Redundant `enable_remove_irrelevant` feature** from Stage 6
  - Removed 69-line `remove_irrelevant_segments()` function
  - Removed config options: `enable_remove_irrelevant`, `irrelevant_threshold`
  - Removed from pipeline.py, __init__.py, display.py
  - Updated all documentation (README, CONFIGURATION, PIPELINE_STAGES)

**Reason:** Feature was 100% redundant with Stage 5's `timing_validation`:
- Both re-transcribe segments with Whisper
- Both remove segments with low similarity
- Stage 5 runs earlier (better position)
- Stage 5 is more efficient (only suspicious segments)
- Removed feature had zero test coverage

**Impact:** Simplified codebase by 95 lines, no loss of functionality.

**Files changed:**
- `modules/stage6_timing_realignment/processor.py` (-69 lines)
- `config.json` (-2 config options)
- `core/pipeline.py`, `core/display.py`
- `docs/` (README, CONFIGURATION, PIPELINE_STAGES)

---

## [2025-10-10 21:28]

### Added
- **Full pipeline E2E test** (`tests/e2e/test_full_pipeline.py`)
  - Complete 9-stage pipeline verification
  - Uses real Japanese audio (counting 1-10)
  - Validates VTT output generation

### Changed
- **Test organization improvements**
  - Moved `test_media/` to `tests/e2e/test_media/` (consistency)
  - Updated all test paths to use relative paths
  - Cleaned up temporary test files
  - Updated E2E README with comprehensive documentation

### Fixed
- **Test file structure**: Flattened nested test_media directory

**Impact:** Complete end-to-end testing coverage, organized test structure.

---

## [2025-10-10 21:00] - Stage 6 Timing Realignment Improvements

### Changed
- **Text similarity algorithm upgraded**
  - Old: Simple character position matching
  - New: `difflib.SequenceMatcher` (Ratcliff/Obershelp algorithm)
  - Improvement: +22% accuracy for Japanese text variations
  - File: `modules/stage6_timing_realignment/utils.py`

- **Search algorithm optimized**
  - Limited to max 5 segment combinations (was unlimited)
  - Added early termination when match found (>0.9 similarity)
  - Added length check to prevent excessive combinations
  - File: `modules/stage6_timing_realignment/text_search_realignment.py`

- **Thresholds adjusted for Japanese**
  - Old: 0.95 (text_search), 1.0 (time_based) - unrealistic
  - New: 0.75 (both methods) - realistic for particle variations
  - Tested with Japanese text: ã“ã‚Œã¯ vs ã“ã‚Œã‚ = 0.667 similarity
  - File: `config.json`

- **Added word-level timestamp matching**
  - New: `find_text_in_words()` fallback function
  - Uses Whisper word timestamps for precise alignment
  - File: `modules/stage6_timing_realignment/utils.py`

### Added
- **E2E tests with real Japanese audio**
  - `tests/e2e/test_timing_realignment.py` - Algorithm tests (23 cases)
  - `tests/e2e/test_realignment_demonstration.py` - Misalignment demo (10 segments)
  - Test audio: Japanese counting 1-10 (167KB, 27 seconds)
  - Results: 8/10 segments adjusted, 0 overlaps (100% fixed)

**Impact:** Significantly improved timing realignment accuracy for Japanese transcription.

---

## [2025-10-10 21:14] - Initial Project

### Added
- Initial 9-stage transcription pipeline
- All core modules and shared utilities
- Configuration system
- Test suite (257 tests)
- Documentation (README, ARCHITECTURE, CONFIGURATION, PIPELINE_STAGES)

---

## Version History Summary

| Date & Time      | Type    | Summary                                           | Tests   |
|------------------|---------|---------------------------------------------------|---------|
| 2025-10-12 07:15 | Changed | Made segment_merging stage optional               | 270     |
| 2025-10-12 07:00 | Changed | Made segment_splitting stage optional             | 270     |
| 2025-10-12 06:45 | Changed | Simplified documentation structure                | 275     |
| 2025-10-12 06:30 | Changed | Complete documentation refactoring (AI guides)    | 275     |
| 2025-10-12 06:00 | Added   | AI_GUIDE.md refactoring plan for scalability      | 275     |
| 2025-10-12 05:30 | Added   | Automatic Ollama installation and management      | 275     |
| 2025-10-12 05:18 | Changed | Simplified stage-specific LLM override            | 275     |
| 2025-10-11 15:00 | Added   | Configurable LLM providers (Ollama, OpenAI)       | 270     |
| 2025-10-11 12:03 | Added   | config.local.json support with deep merge         | 261     |
| 2025-10-11 11:58 | Changed | Streamlined README.md for user focus              | 261     |
| 2025-10-11 11:56 | Added   | docs/features/ folder for stage documentation     | 261     |
| 2025-10-11 11:50 | Changed | Restructured docs for Claude Code compatibility   | 261     |
| 2025-10-11 11:40 | Added   | API key security documentation and gitignore      | 261     |
| 2025-10-11 11:37 | Added   | MIT License and secured API keys                  | 261     |
| 2025-10-11 02:02 | Changed | Moved session history to docs/SESSIONS.md         | 261     |
| 2025-10-11 01:55 | Changed | Streamlined AI_GUIDE.md to remove redundancy      | 261     |
| 2025-10-11 01:50 | Added   | CHANGELOG.md and documentation guidelines         | 261     |
| 2025-10-11 01:48 | Fix     | Stage 5 re-filtering after timing_validation      | 261     |
| 2025-10-11 01:30 | Added   | AI_GUIDE.md for AI assistant continuity           | 257     |
| 2025-10-11 01:16 | Removed | Redundant enable_remove_irrelevant feature        | 257     |
| 2025-10-10 21:28 | Added   | Full pipeline E2E test + test organization        | 257     |
| 2025-10-10 21:14 | Added   | Initial 9-stage pipeline project                  | 257     |

---

## How to Update This Changelog

When making significant changes:

1. **Add entry at the top** with current Date + Time in format [YYYY-MM-DD HH:MM]
2. **Use clear categories**: Added, Changed, Fixed, Removed
3. **Explain the impact** - why this change matters
4. **List affected files** - help future developers find code
5. **Update the Version History Summary table** with the new entry

**Example entry:**
```markdown
## [2025-01-12 14:30]

### Added
- New Japanese particle normalization in Stage 7
  - Handles ã¯/ã‚, ã‚’/ãŠ, ã¸/ãˆ variations
  - File: `modules/stage7_text_polishing/normalizer.py`

**Impact:** Improved text consistency for Japanese particles.
```

---

*This changelog tracks all significant changes to the transcribe-jp project. For detailed commit history, use `git log`.*
