{
  // This is an example config.local.json file for local overrides
  // Copy this to config.local.json and customize (it's gitignored)
  // Only include settings you want to override from config.json

  "whisper": {
    "device": "cuda"
  },

  "llm": {
    // Choose provider: "ollama" (free, local, auto-managed), "anthropic" (paid, high quality), or "openai" (paid)
    "provider": "anthropic",

    // "max_tokens": 0,       // Optional: 0 = unlimited (recommended for large batches)
    // "timeout": 60,         // Optional: Global timeout in seconds (default: 30s)

    "ollama": {
      "model": "llama3.2:3b"
      // "base_url": "http://localhost:11434"  // Optional: Use external Ollama server
      // "executable_path": "D:\\CustomPath\\ollama.exe"  // Optional: Custom Ollama installation path
      // "timeout": 45         // Optional: Override timeout for Ollama (useful for large models)
      // "context_length": 2048  // Optional: Reduce context window to save VRAM (0 = model default, typically 2K-32K)
    },

    "anthropic": {
      "api_key": "sk-ant-api03-your-actual-key-here",
      "model": "claude-3-5-haiku-20241022"
    },

    "openai": {
      "api_key": "sk-proj-your-actual-key-here",
      "model": "gpt-4o-mini"
    }
  },

  "segment_merging": {
    "enable": true       // Set to false to skip merging incomplete sentences
  },

  "segment_splitting": {
    "enable": true,      // Set to false to skip splitting long segments
    "enable_llm": true
    // "llm_provider": "ollama"  // Uncomment to override global provider for this stage
  },

  "text_polishing": {
    "enable": true,
    "batch_size": 10    // Set to 1 to process one-by-one (recommended for Ollama)
    // "llm_provider": "anthropic"  // Uncomment to override global provider for this stage
    // "llm_timeout": 120   // Optional: Override timeout just for this stage (useful for large models)
  },

  "timing_realignment": {
    "enable": true
  }

  // ============================================================================
  // EXAMPLE CONFIGURATIONS (remove this section in your actual config.local.json)
  // ============================================================================

  // Example 1: FREE Ollama only
  // {
  //   "llm": {
  //     "provider": "ollama",
  //     "ollama": {
  //       "model": "llama3.2:3b"
  //     }
  //   }
  // }

  // Example 2: Anthropic only
  // {
  //   "llm": {
  //     "provider": "anthropic",
  //     "anthropic": {
  //       "api_key": "sk-ant-..."
  //     }
  //   }
  // }

  // Example 3: Hybrid - FREE Ollama for splitting, paid Claude for polishing
  // {
  //   "llm": {
  //     "provider": "ollama",
  //     "ollama": {
  //       "model": "llama3.2:3b"
  //     },
  //     "anthropic": {
  //       "api_key": "sk-ant-..."
  //     }
  //   },
  //   "segment_splitting": {
  //     "enable_llm": true
  //     // Uses global "ollama" provider (FREE)
  //   },
  //   "text_polishing": {
  //     "enable": true,
  //     "llm_provider": "anthropic"  // Override to use Anthropic for this stage
  //   }
  // }

  // Example 4: Custom Ollama installation path
  // {
  //   "llm": {
  //     "provider": "ollama",
  //     "timeout": 90,
  //     "ollama": {
  //       "model": "qwen3:8b",
  //       "executable_path": "D:\\CustomApps\\Ollama\\ollama.exe",  // For non-standard installations
  //       "context_length": 2048  // Reduce VRAM usage for large models (default: model-specific, typically 2K-32K)
  //     }
  //   },
  //   "text_polishing": {
  //     "batch_size": 1,
  //     "llm_timeout": 180  // 3 minutes for larger model
  //   }
  // }

  // For detailed LLM configuration guide, see: docs/features/LLM_PROVIDERS.md
  // For advanced Ollama configuration: docs/features/OLLAMA_CONFIGURATION.md
}
